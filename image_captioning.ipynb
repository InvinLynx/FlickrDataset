{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"image_captioning.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"7k3GdDHkSi8O","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LEuoWVHyS65t","colab_type":"code","colab":{}},"source":["import numpy as np \n","import pandas as pd \n","\n","import os\n","for dirname, _, filenames in os.walk('/content/drive/My Drive/Flickr_Data'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tBPmesvjq3Pk","colab_type":"code","colab":{}},"source":["#!pip install matplotlib\n","import matplotlib.pyplot as plt\n","#!pip install tensorflow==2.0\n","%matplotlib inline\n","from pickle import dump\n","import tensorflow.keras as keras\n","from keras.preprocessing.image import load_img\n","from keras.preprocessing.image import img_to_array\n","from keras.applications.vgg16 import VGG16\n","from keras.applications.resnet50 import ResNet50\n","from keras.applications.resnet50 import preprocess_input\n","# for using pretrained models and deploying the new model acc. to the needs\n","from keras.models import Model\n","import h5py\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VLSlDOFBrGSZ","colab_type":"code","colab":{}},"source":["# mapping the required versions\n","!pip show tensorflow \n","!pip list | grep keras\n","\n","import os\n","\n","!pip show keras\n","!pip show tensorflow\n","!pip show sklearn\n","!pip show pillow\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"urRoBs11rSXc","colab_type":"code","colab":{}},"source":["# distributing the file dir data and generating the sequence datagram file \n","# by using pre-trained open community VGGNet algorithm\n","\n","print(os.listdir('../content/drive/My Drive/Flickr_Data'))\n","\n","root, dirs, imgs = next(os.walk('../content/drive/My Drive/Flickr_Data/Images', topdown=True))\n","print(root)\n","\n","path, dir, directory = next(os.walk('../content/drive/My Drive/Flickr_Data/Flickr_TextData'))\n","print(imgs,\"\\n\")\n","print(directory)\n","\n","imgCount = len(imgs)\n","dirCount = len(directory)\n","\n","print(\"Image Counted : %d\" % imgCount)\n","print(\"Directory Iterated : %d\" %dirCount)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HtGYk_VPrljU","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import LabelEncoder\n","import random\n","import PIL\n","import tensorflow.keras as keras\n","from tensorflow.keras.preprocessing.image import load_img\n","from tensorflow.keras.preprocessing.image import img_to_array\n","\n","# bursting the loaded images in the random order\n","img_path = '../content/drive/My Drive/Flickr_Data/Images/'\n","max_img = len(imgs)\n","overRand = random.randint(0,max_img)\n","print(\"Input-Image ID picked: \",overRand)\n","print(\"Initial Input : %s\" %imgs[overRand])\n","image = load_img(img_path + imgs[overRand])\n","image\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i6vtPPMir98E","colab_type":"code","colab":{}},"source":["# seeking the shape of the input image\n","image = img_to_array(image)\n","image.shape\n","print(image)\n","\n","# reshaping the data into single sample image and preparing the sample for the VGGNet16( pre-trained network)\n","\n","from keras.applications.vgg16 import preprocess_input\n","image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n","resnet_image = preprocess_input(image)\n","print(resnet_image)\n","resnet_image.shape\n","\n","image_res = load_img(img_path + imgs[overRand], target_size=(255,255))\n","image_res\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hm2RQ87VsaBV","colab_type":"code","colab":{}},"source":["# using the predefined model structure to extract features corresponding to each image\n","\n","def featuresExt(fileSrc):\n","    # restructing the resnet50 model\n","    modelMap = ResNet50()\n","    modelMap.layers.pop()\n","    \n","    modelMap = Model(inputs = modelMap.inputs, outputs = modelMap.layers[-1].output)\n","    \n","    print(modelMap.summary())\n","    \n","    # extracting the features from the images and storing in unzip dict()\n","    \n","    traceExt = dict()\n","    \n","    for file in os.listdir(fileSrc):\n","        filename = fileSrc  +  '/' + file\n","        image = load_img(filename, target_size=(224,224))\n","        image = img_to_array(image)\n","        \n","        image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n","        \n","        # parsing the image for resnet50 model training\n","        image = preprocess_input(image)\n","        imageId = file.split('.')[0]\n","        \n","        # extracting the features out of the images\n","        trace = modelMap.predict(image, verbose = 1)\n","        traceExt[imageId] = trace\n","        \n","        print(\"Image read > %s\" %file)\n","        \n","    return traceExt\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GaGdJ89BsgIs","colab_type":"code","colab":{}},"source":["fileSrc = \"../content/drive/My Drive/Flickr_Data/Images/\"\n","traces = featuresExt(fileSrc)\n","\n","print(\"In-total features extracted for the supplied Image dataset : %d\" %len(traces))\n","\n","# saving the output to the traceExt file\n","#file = open(traceExt.pkl, 'wb')\n","dump(traces, open('traceExt.pkl', 'wb'))\n","\n","\n","!head -n 100 traceExt.pkl\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PAYY7jP9stOy","colab_type":"code","colab":{}},"source":["!pip show tensorflow \n","!pip show keras\n","!pip show numpy\n","!pip show pickle\n","!pip show matplotlib\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j5a54zCVdx_Y","colab_type":"code","colab":{}},"source":["from numpy import array\n","from pickle import load\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","# initial all necessary workflow library needed according to Tf 1.x again\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.utils import plot_model\n","from keras.models import Model\n","from keras.layers import Dense\n","from keras.layers import Input\n","from keras.layers import LSTM\n","from keras.layers import Embedding \n","from keras.layers import Dropout\n","from keras.layers.merge import add\n","from keras.callbacks import ModelCheckpoint # for plotting the loss-access function tabular graph\n","import tensorflow.keras as keras\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IWsxzuk3eaHv","colab_type":"code","colab":{}},"source":["# making the instance of data in the memory\n","\n","def loadDoc(file):\n","    fl = open(file, 'r')\n","    text = fl.read()\n","    fl.close()\n","    \n","    return text\n","\n","# loading the pre-trained photo identifier\n","\n","def loadData(file):\n","    data = loadDoc(file)\n","    dataset = list()\n","    \n","# iterating the pre-trained identifier and get the image identifier\n","    \n","    for instance in data.split('\\n'):\n","        if len(instance) < 1:\n","            continue\n","            \n","        identifier = instance.split('.')[0]\n","        dataset.append(identifier)\n","        \n","    return set(dataset)\n","\n","# loading the pre-trained description image identifier\n","\n","def loadDesc(file, dataset):\n","    data = loadDoc(file)\n","    description = dict()\n","    # iterate the data and split the description where white space encountered\n","    for line in data.split('\\n'):\n","        token = line.split()\n","        image_id, image_desc = token[0], token[1:]\n","        # map the image identifier with the description data \n","        if image_id in dataset:\n","            if image_id not in description:\n","                description[image_id] = list()\n","            # wrapping the description to instance token    \n","            desc = 'startseq'+ ' '.join(image_desc) + 'endseq'\n","            # store the desc with the corresponding image_id\n","            description[image_id].append(desc)\n","                    \n","    return description\n","\n","# converting the clean dictionary description to list format\n","\n","def desc_lines(description):\n","    list_desc = list()\n","    for key in description.keys():\n","        for desc in description[key]:\n","            list_desc.append(desc)\n","            \n","    return list_desc\n","\n","# calculating the description length with most occuring words\n","\n","def desc_length(description):\n","    lines = desc_lines(description)\n","    \n","    return max(len(desc.split()) for desc in lines)\n","\n","# loading image features for model training\n","\n","def load_features(file, dataset):\n","    features_all = load(open(file, 'rb'))\n","    # store the mapped features along with image_id into dict\n","    feature = {image : features_all[image] for image in dataset}\n","    \n","    return feature\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EOUCqHhRenGc","colab_type":"code","colab":{}},"source":["# modelling sequence acc. to the paper algorithm and train-model derivation using Encoder-Decoder Architecture\n","\n","# creating the image sequence model with input sequences and output words\n","\n","def initiate_sequences(tokenizer, desc_length, desc_list, photo):\n","    a, b, y = list(), list(), list()\n","    \n","    #traversing each description for output image\n","    for desc in desc_list:\n","        # encoding the sequence\n","        seq = tokenizer.texts_to_sequences([desc])[0]\n","        # split the sequence into multiple instance pair\n","        for i in range(1, len(seq)):\n","            # split into input and output pairs\n","            input_seq, output_seq = seq[:i], seq[i]\n","            input_seq = pad_sequences([input_seq], maxlen= desc_length)[0]\n","            #encode the output sequence\n","            output_seq = to_categorical([output_seq],num_classes=vocabulary_size)[0]\n","            # store the experimental outputs\n","            a.append(photo)\n","            b.append(input_seq)\n","            y.append(output_seq)\n","     # importing the array obj method from numpy       \n","    return array(a), array(b), array(y)\n","\n","# defining the realedge captioning model\n","\n","def caption_model(vocabulary_size, desc_length):\n","\n","# initialising the feature extractors\n","    inputs1 = Input(shape=(2048,))\n","    feat_ext1 = Dropout(0.5)(inputs1)\n","    feat_ext2 = Dense(256, activation='relu')(feat_ext1)\n","    # initialinsing the sequence extractor\n","    inputs2  = Input(shape=(desc_length,))\n","    seq_ext1 = Embedding(vocabulary_size, 256, mask_zero=True)(inputs2)\n","    seq_ext2 = Dropout(0.5)(seq_ext1)\n","    seq_ext3 = LSTM(256)(seq_ext2) # using the LSTM gates logic for storing data\n","    # defining the 3-layer decoder model over the initialised extractors\n","    #dec_model = add(Flatten(input_shape = (52,52,1)))\n","    dec_model1 = add([feat_ext2, seq_ext3])\n","    dec_model2 = Dense(256, activation='relu')(dec_model1)\n","    dec_model3 = Dropout(0.5)(dec_model2) # lr =0.5\n","    \n","    outputs = Dense(vocabulary_size, activation='softmax')(dec_model3)\n","    # publish the [input,[image,[words]]] together in model\n","    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","    # performing the compiling task \n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    # ploting the model output and making sumarization of the flown input seq\n","    model.summary()\n","    plot_model(model, to_file='model_shape.png', show_shapes=True) # to feature the flow stances of the training model\n","    \n","    return model\n","\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GJttVIt5exTf","colab_type":"code","colab":{}},"source":["# visualization block activity tokens\n","# token generation and datagram block for clean captioning\n","\n","# now fitting the tokenizer to the image captioning\n","\n","from keras.callbacks import ModelCheckpoint\n","import tensorflow as tf \n","\n","def fit_tokenizer(description):\n","    fit_line = desc_lines(description)\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(fit_line)\n","    return tokenizer\n","\n","# data generator- the predefined method used in call to model.fit_generator()\n","\n","def data_generator(description, photos, tokenizer, desc_length):\n","    # loop over images\n","    while 1:\n","        for key, desc_list in description.items():\n","            #reteiving the photo features\n","            photo = photos[key][0]\n","            in_img, in_seq, out_img = initiate_sequences(tokenizer, desc_length, desc_list, photo) # pre-defined over-rulling function\n","            yield [[in_img,in_seq], out_img]\n","                 \n","\n","# visualization - defining the class value method to trace the training and epochs activity\n","\n","class valueLosses(tf.keras.callbacks.Callback):\n","    \n","    def begin_train(self, logs={}):\n","        self.i = 0\n","        self.x = []\n","        self.loss = []\n","        self.loss_values = []\n","        self.figure = plt.figure()\n","        self.logs = []\n","        \n","    def end_epochs(self, epoch, logs={}):\n","        self.logs.append(logs)\n","        self.loss.append(logs.get('loss'))\n","        self.x.append(self.i)\n","        self.loss_values.append(logs.get('val_loss'))\n","        self.i += 1\n","        \n","        # throwing the plots corresponding to the loss and val_loss for training offset and for the defined model epochs\n","        plt.plot(self.x, self.loss, label='loss')\n","        plt.plot(self.x, self.loss_values, label='val_loss')\n","        plt.legend()\n","        plt.show()\n","        \n","losses = valueLosses()\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o532sWHWe3ZN","colab_type":"code","colab":{}},"source":["# initiating the model training \n","# loading the 8kflickcfeature trainImage dataset\n","\n","file = '../content/drive/My Drive/Flickr_Data/Flickr_TextData/Flickr_8k.trainImages.txt'\n","train = loadData(file)\n","print(\"Training Data:\")\n","print(train)\n","print(\"Dataset loaded : %d\" % len(train))\n","# loading the clean description corresponding to the training data\n","desc_train = loadDesc('../content/drive/My Drive/Flickr_Data/descriptions.txt', train)\n","print(\"Description corresponding to training data:\")\n","print(desc_train)\n","print(\"Description on-Train : %d\" %len(desc_train))\n","file = '../content/drive/My Drive/Flickr_Data/Flickr_TextData/Flickr_8k.devImages.txt'\n","validate = loadData(file)\n","print(\"Validating Data:\")\n","print(validate)\n","print('Validated data loaded : %d' %len(validate))\n","desc_validation = loadDesc('../content/drive/My Drive/Flickr_Data/descriptions.txt', validate)\n","print(\"Description corresponding to Validation Data:\")\n","print(desc_validation)\n","print('Description on-Validation : %d' %len(desc_validation))\n","\n","# extracting features trained using the image dataset and loading the pkl file\n","\n","ftr_train = load_features('../content/drive/My Drive/Flickr_Data/traceExt.pkl', train)\n","print(\"Traced Features Extracted\")\n","print(ftr_train)\n","\n","print(\"Model trained over features(Training Photos) : %d\" %len(ftr_train))\n","\n","ftr_validate = load_features('../content/drive/My Drive/Flickr_Data/traceExt.pkl', validate)\n","print(ftr_validate)\n","\n","print(\"Model validation over features(Testing photos) : %d\" %len(ftr_validate))\n","\n","\n","# creating the tokens from the trained features for the better testing results\n","\n","tokenizer = fit_tokenizer(desc_train)\n","print(\"Tokenizer for the Clean Descriptions:\")\n","print(tokenizer)\n","\n","# size of the vocabulary/ tokens\n","vocabulary_size = len(tokenizer.word_index) + 1\n","print(\"Vocabulary size for the Image Dataset :%d \" % vocabulary_size)\n","\n","# maximum sequence length for the trained data\n","max_length = desc_length(desc_train)\n","#print(max_desc)\n","print(\"Sequence Length of the Data : %d\" %(max_length))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KHDOvdxTfiVo","colab_type":"code","colab":{}},"source":["# defining the arguments for training the ResNet50 over the VGG16 features predictions\n","# stating the elemental sources to efficiently and wisely train the model for better predictions with steepest accuracy\n","\n","model = caption_model(vocabulary_size, max_length)\n","# defining the least epochs for revised training of the ResNet50\n","#print(len(desc_train))\n","\n","steps = len(desc_train)\n","val_steps = len(desc_validation)\n","\n","generator = data_generator(desc_train, ftr_train, tokenizer, max_length)\n","val_generator = data_generator(desc_validation, ftr_validate, tokenizer, max_length)\n","\n","# creating the full fledged model and creating the History.history object\n","history = model.fit_generator(generator, epochs=31, steps_per_epoch=steps, verbose=1, callbacks=[losses], validation_data=val_generator, validation_steps=val_steps)\n","    \n","    # saving the models trained over features in mutliple proceedings\n","    # to further perform evaluations and prediction verbosity\n","    \n","model.save_weights(\"featureModel_ResNet.h5\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YLRsBpjJhaAW","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}